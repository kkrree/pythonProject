{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble 모형결합\n",
    "\n",
    "1. Bagging   \n",
    "같은 모형, 같은 데이터 샘플을 중복 사용하여 서로 다른 결과를 출력한 다수의 모형을 사용하는 방법\n",
    " * BaggingClassifier : 배깅 모형 결합을 위한 클래스\n",
    " * base_estimator : 기본 모형\n",
    " * n_estimators : 모형 갯수 (default 10)\n",
    " * bootstrap_features : 특징 차원의 중복 사용 여부 (default False)\n",
    " * max_features : 다차원 독립 변수 중 선택할 차원의 수 혹은 비율 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier(random_state=0)\n",
    "model2 = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=0)\n",
    "\n",
    "for model in (model1,model2):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train) # train_test_split 했다고 가정\n",
    "    print('학습용: ', model.score(X_train, y_train))\n",
    "    print('검증용: ', model.score(X_test, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. Random Forest   \n",
    "- 의사결정나무를 개별 모형으로 사용하는 모형 결합 방법   \n",
    "- 배깅은 사용하는 모형의 종류에 제한이 없으나, 랜덤포레스트는 의사결정나무 모형만을 사용\n",
    "- 독립변수의 차원을 랜덤하게 감소시킨 후 독립변수를 선택하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier(random_state=0)\n",
    "model2 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "for model in (model1,model2):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train) # train_test_split 했다고 가정\n",
    "    print('학습용: ', model.score(X_train, y_train))\n",
    "    print('검증용: ', model.score(X_test, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+) Extremely Randomized Trees\n",
    "- 랜덤포레스트의 변정으로, Extra Trees라고도 한다\n",
    "- 독립변수를 무작위로 선택하는 방식\n",
    "- 랜덤포레스트는 DecisionTreeClassifier를 사용하지만, 엑스트라 트리는 ExtraTressClassifier(DecisionTreeClassifier를 상속한 클래스)를 사용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model3 = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "for model in (model1,model2,model3):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train) # train_test_split 했다고 가정\n",
    "    print('학습용: ', model.score(X_train, y_train))\n",
    "    print('검증용: ', model.score(X_test, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> DecisionTree < RandomForest < ExtraTree 순으로 성능이 개선됨\n",
    "트리를 만드는 결정에 각 특성이 얼마나 중요한지를 평가하는 특성 중요도 적용   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "053d2992ea4a1306d75dbbbe749afeb63994e3a3213349c301f382405d7edb98"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('virt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
