{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble 모형결합\n",
    "\n",
    "1. Bagging   \n",
    "같은 모형, 같은 데이터 샘플을 중복 사용하여 서로 다른 결과를 출력한 다수의 모형을 사용하는 방법\n",
    " * BaggingClassifier : 배깅 모형 결합을 위한 클래스\n",
    " * base_estimator : 기본 모형\n",
    " * n_estimators : 모형 갯수 (default 10)\n",
    " * bootstrap_features : 특징 차원의 중복 사용 여부 (default False)\n",
    " * max_features : 다차원 독립 변수 중 선택할 차원의 수 혹은 비율 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier(random_state=0)\n",
    "model2 = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=0)\n",
    "\n",
    "for model in (model1,model2):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train) # train_test_split 했다고 가정\n",
    "    print('학습용: ', model.score(X_train, y_train))\n",
    "    print('검증용: ', model.score(X_test, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. Random Forest   \n",
    "- 의사결정나무를 개별 모형으로 사용하는 모형 결합 방법   \n",
    "- 배깅은 사용하는 모형의 종류에 제한이 없으나, 랜덤포레스트는 의사결정나무 모형만을 사용\n",
    "- 독립변수의 차원을 랜덤하게 감소시킨 후 독립변수를 선택하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model1 = DecisionTreeClassifier(random_state=0)\n",
    "model2 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "for model in (model1,model2):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train) # train_test_split 했다고 가정\n",
    "    print('학습용: ', model.score(X_train, y_train))\n",
    "    print('검증용: ', model.score(X_test, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+) Extremely Randomized Trees\n",
    "- 랜덤포레스트의 변정으로, Extra Trees라고도 한다\n",
    "- 독립변수를 무작위로 선택하는 방식\n",
    "- 랜덤포레스트는 DecisionTreeClassifier를 사용하지만, 엑스트라 트리는 ExtraTressClassifier(DecisionTreeClassifier를 상속한 클래스)를 사용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model3 = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "for model in (model1,model2,model3):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train) # train_test_split 했다고 가정\n",
    "    print('학습용: ', model.score(X_train, y_train))\n",
    "    print('검증용: ', model.score(X_test, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DecisionTree < RandomForest < ExtraTree 순으로 성능이 개선됨   \n",
    "> 트리를 만드는 결정에 각 특성이 얼마나 중요한지를 평가하는 특성 중요도 적용   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. Boosting   \n",
    "- 미리 정해진 갯수의 모형을 사용하지 않고 하나의 모형에서 시작하여 개별 모형을 하나씩 추가하는 방법   \n",
    "- 다수결 방법이 아닌 개별 모형의 출력에 가중치를 조합한 값을 판별함수로 사용\n",
    "\n",
    "- base_estimetor \n",
    "    - 부스트 앙상블이 만들어지는 기본 분류 모형\n",
    "    - 기본값 DecisionTreeClassifier (max_depth = 1)\n",
    "- n_estimators \n",
    "    - 기본값 = 50\n",
    "    - 부스팅이 종료되는 최대 추정량 (조기 종료)\n",
    "- learning_rate \n",
    "    - 기본값 = 1\n",
    "    - 학습진행 속도\n",
    "\n",
    "\n",
    " 3-1. Adaboost    \n",
    "- adaptive boost(적응형 부스트)\n",
    "- 학습 데이터에 가중치를 주고 분류 모형이 틀리게 예측한 데이터의 가중치를 합한 값을 손실함수로 사용하고 손실함수를 최소화하는 모형을 선택하는 알고리즘\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model_ada = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "model_ada.fit(X_train, y_train) # train_test_split 했다고 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- algorithm \n",
    "    - SAMME\n",
    "    - SAMME.R(default = 'SAMME.R') : SAMME보다 빠르게 수렴되므로 부스팅 반복 횟수가 줄어 테스트 오류가 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(probability=True, kernel='linear')\n",
    "model_svc = AdaBoostClassifier(algorithm='SAMME', n_estimators=50, base_estimator=svc)\n",
    "model_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-2. Gradientboost \n",
    "- 손실을 최소화하는 개별 분류함수를 찾는 알고리즘\n",
    "- 내부적으로 의사결정 회귀나무 모형을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_grad = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-3. XGBoost\n",
    "- 그레디언트 부스트 알고리즘을 분산환경에서도 실행할 수 있도록 구현된 라이브러리\n",
    "- 성능이 우수하고 자원 활용률이 좋아서 최근 많이 사용되고 있는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "model_xgb = xgboost.XGBClassifier(n_estimators=100, max_depth=3, random_state=0)\n",
    "%%time ##??\n",
    "model_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-4. lightgbm\n",
    "- GOSS(Gradient-based One_side Sampling)을 통해 데이터의 일부만으로 빠르게 정보이득을 계산하고 EFM(Exclusive Feature Bundling)을 통해 Feature를 획기적으로 감소시켜서 XGBoost보다 더 좋은 성능을 나타내는 알고리즘\n",
    "- 정보이득(어떤 속성을 선택함으로 인해 데이터를 더 잘 구분하게 되는 특성)\n",
    "    - 범주형 변수를 One Hot Incoding을 하게 되면 변수가 많이 늘어나서 계산이 오래 걸리는 상황이 발생하는데 lightgbm은 이 문제를 잝 극복할 수 있는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "053d2992ea4a1306d75dbbbe749afeb63994e3a3213349c301f382405d7edb98"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('virt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
